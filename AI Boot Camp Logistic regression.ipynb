{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "AI Bootcamp 2022 Winter\n",
    "\n",
    "Dates: 2022-2-7\n",
    "\n",
    "Author: Yung-Kyun Noh\n",
    "\n",
    "Department of Computer Science, Hanyang University\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_state(wval, bval, title_str='Data'):\n",
    "    # function for scattering data and drawing classification boundary\n",
    "    # wx - b > 0 or  wx - b < 0\n",
    "    \n",
    "    # create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Scatter data points in 2-dimensional space\n",
    "    ax.scatter(data1[:,0], data1[:,1], label='class 1', c='red', alpha=.3)\n",
    "    ax.scatter(data2[:,0], data2[:,1], label='class 2', marker='^', c='blue', alpha=.3)\n",
    "    # set a title and labels\n",
    "    ax.set_title(title_str)\n",
    "    ax.legend()\n",
    "    \n",
    "    [x1min,x1max,x2min,x2max] = ax.axis()\n",
    "    x1vals = np.arange(x1min,x1max,0.1)\n",
    "    ax.plot(x1vals, (-wval[0]*x1vals + bval)/wval[1], 'k')\n",
    "    ax.axis([x1min,x1max,x2min,x2max])\n",
    "    ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(t):\n",
    "    # example: logistic_function(np.array([0,1,2]))\n",
    "    \n",
    "    ret_val = 1/(1 + np.exp(-t))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(labels, fs):\n",
    "    loss_val = np.sum(labels*np.log(fs) + (1 - labels)*np.log(1 - fs))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two Gaussians (class 1 & class 2)\n",
    "dim = 2\n",
    "datanum1 = 50\n",
    "datanum2 = 50\n",
    "mean1 = np.array([0, 0])\n",
    "mean2 = np.array([1, -.5])\n",
    "cov1 = np.array([[.1,.02],[.02,.1]])\n",
    "cov2 = np.array([[.1,.02],[.02,.1]])\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, datanum1)\n",
    "data2 = np.random.multivariate_normal(mean2, cov2, datanum2)\n",
    "tstdatanum1 = 100\n",
    "tstdatanum2 = 100\n",
    "tstdata1 = np.random.multivariate_normal(mean1, cov1, tstdatanum1)\n",
    "tstdata2 = np.random.multivariate_normal(mean2, cov2, tstdatanum2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Bayes classifier\n",
    "\n",
    "When the data generating functions are Gaussians having the equivalent covariances, we can obtain the optimal linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal linear classifier\n",
    "optw = np.matmul(mean1 - mean2, np.linalg.inv(cov1))\n",
    "optb = np.matmul(optw, (mean1 + mean2)/2)\n",
    "print(optw, optb)\n",
    "\n",
    "draw_state(optw, optb, 'Data and optimal boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w_init = np.random.normal(0,1,dim)\n",
    "b_init = np.random.normal(0,1,1)\n",
    "\n",
    "# extended w: [w, -b]\n",
    "extw = np.array([np.concatenate((w_init, -b_init))])\n",
    "# data with '1' is appended: [X, 1]\n",
    "extX = np.concatenate((np.concatenate((data1, data2), axis=0), \\\n",
    "                       np.ones([datanum1 + datanum2, 1])), axis=1)\n",
    "labels = np.concatenate((np.ones(datanum1), np.zeros(datanum2)))  # label of class 1: 1, label of class 2: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(extw, extX, labels, num_update=100, step_size=0.001, reg_const=1):\n",
    "#     print(extw)\n",
    "    draw_state(extw[0,0:2], -extw[0,2], 'Before update')\n",
    "\n",
    "    objective_history = []\n",
    "    for i in range(num_update):\n",
    "        ts = np.matmul(extX, extw.T)  # w^TX\n",
    "        fs = logistic_function(ts)\n",
    "        extw = extw + step_size*(np.matmul(np.array([labels]) - fs.T, extX) - reg_const*extw)\n",
    "        objective_history.append(get_loss(labels, fs.T[0]))\n",
    "\n",
    "    draw_state(extw[0,0:2], -extw[0,2], 'Updated boundary')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n",
    "    \n",
    "    return extw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extw = update_w(extw, extX, labels, num_update=500, step_size=0.001, reg_const=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional 100 updates\n",
    "extw = update_w(extw, extX, labels, num_update=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "extTstX = np.concatenate((np.concatenate((tstdata1, tstdata2), axis=0), \\\n",
    "                       np.ones([tstdatanum1 + tstdatanum2, 1])), axis=1)\n",
    "TstLabels = np.concatenate((np.ones(tstdatanum1), np.zeros(tstdatanum2)))\n",
    "\n",
    "ts = np.matmul(extTstX, extw.T)  # w^TX\n",
    "err_rate = np.sum(np.abs((ts.T > 0) - np.array([TstLabels])))/(tstdatanum1 + tstdatanum2)\n",
    "print(err_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "x_train = np.concatenate((data1, data2), axis=0)\n",
    "y_train = np.concatenate((np.ones(datanum1), np.zeros(datanum2)))  # label of class 1: 1, label of class 2: 0\n",
    "x_test = np.concatenate((tstdata1, tstdata2), axis=0)\n",
    "y_test = np.concatenate((np.ones(tstdatanum1), np.zeros(tstdatanum2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((data1, data2), axis=0)\n",
    "y_train = np.concatenate((np.ones(datanum1), np.zeros(datanum2)))  # label of class 1: 1, label of class 2: 0\n",
    "x_test = np.concatenate((tstdata1, tstdata2), axis=0)\n",
    "y_test = np.concatenate((np.ones(tstdatanum1), np.zeros(tstdatanum2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1, activation='sigmoid', input_shape=(2,), kernel_regularizer=keras.regularizers.l2(0.1)))   # caution: not 'softmax' or 'relu'\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=500,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], 'b', label='train loss')\n",
    "ax.plot(history.history['val_loss'], 'r', label='test loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w param: \\n', model.weights[0], '\\n\\nb param: \\n', model.weights[1])\n",
    "\n",
    "draw_state(model.weights[0].numpy(), -model.weights[1].numpy(), 'keras boundary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: calculate the derivative of a function with respect to a vector\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{w}) &=& \\mathbf{w}^\\top \\mathbf{x} \\in\\mathbb{R} \\quad \\text{for} \\quad \\mathbf{w}, \\mathbf{x} \\in \\mathbb{R}^2 \\\\\n",
    "&& \\text{then} \\quad \\frac{df}{d\\mathbf{w}} = ?\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.constant([[2],[3]], dtype='float'))\n",
    "w = tf.Variable(tf.constant([[0],[0]], dtype='float'))\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.matmul(tf.transpose(w), x)\n",
    "\n",
    "df_dw = tape.gradient(f, w)\n",
    "print('derivative=', df_dw.numpy().T, '\\nx=', x.numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking gradients of your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of outputs\n",
    "\n",
    "x = np.array([[1,0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    model = Sequential([\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    fx = model(x)\n",
    "\n",
    "df_dw = tape.gradient(fx, model.weights)   # Derivative of (f at x) w.r.t. weights at the current weights\n",
    "print('f=', f)\n",
    "print('\\ndf/dw=\\n', df_dw[0].numpy(), ' for w,\\nand\\n', df_dw[1].numpy(), 'for b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of losses\n",
    "# gradient for trainable_variables\n",
    "\n",
    "layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "x = x_train\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    y = layer(x)   # weights are initialized here\n",
    "    loss = tf.reduce_mean((tf.transpose(y) - y_train)**2 )\n",
    "    # Calculate gradients with respect to every trainable variable\n",
    "    grad = tape.gradient(loss, layer.trainable_variables)\n",
    "\n",
    "print('loss:', loss)\n",
    "print('\\ngrad for w:\\n', grad[0]) # for w and b separately\n",
    "print('\\ngrad for b:\\n', grad[1]) # for w and b separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression learning\n",
    "\n",
    "def tf_get_loss(labels, fs):\n",
    "    loss_val = -1*tf.math.reduce_sum(labels*tf.math.log(fs) + (1 - labels)*tf.math.log(1 - fs))\n",
    "    return loss_val\n",
    "\n",
    "layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "n_iteration = 500\n",
    "step_size = 0.001\n",
    "reg_const = 0.05\n",
    "for iiter in range(n_iteration):\n",
    "    with tf.GradientTape() as tape:\n",
    "        ys = layer(x_train)   # weights are initialized at first call\n",
    "        loss = tf_get_loss(y_train, tf.transpose(ys)[0])\n",
    "\n",
    "    # Calculate gradients with respect to every trainable variable\n",
    "    grad = tape.gradient(loss, layer.trainable_variables)\n",
    "    if iiter == 0:\n",
    "        print('weights at start:\\nw:\\n', layer.weights[0].numpy(), '\\nb:\\n', layer.weights[1].numpy(), '\\n')\n",
    "\n",
    "    # Update the weights in our linear layer.\n",
    "    layer.weights[0].assign(layer.weights[0] - step_size*grad[0] - reg_const*layer.weights[0])\n",
    "    layer.weights[1].assign(layer.weights[1] - step_size*grad[1])\n",
    "    if iiter % 100 == 0:\n",
    "        print('loss:', loss.numpy())\n",
    "\n",
    "print('\\nweights at finish:\\nw:\\n', layer.weights[0].numpy(),'\\nb:\\n', layer.weights[1].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w param: \\n', layer.weights[0].numpy(), '\\n\\nb param: \\n', layer.weights[1].numpy())\n",
    "\n",
    "draw_state(layer.weights[0].numpy(), -layer.weights[1].numpy(), 'keras boundary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_shape=(2,)))\n",
    "model.add(Dense(units = 512, activation='relu'))\n",
    "model.add(Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=500,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], 'b', label='train loss')\n",
    "ax.plot(history.history['val_loss'], 'r', label='test loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
